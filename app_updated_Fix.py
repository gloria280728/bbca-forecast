import os
import pickle
import numpy as np
import pandas as pd
import streamlit as st
import torch
import requests
import sys
import types

from pytorch_forecasting.data.encoders import GroupNormalizer
from pytorch_forecasting import TemporalFusionTransformer
from pytorch_forecasting.data import TimeSeriesDataSet


st.set_page_config(
    page_title="BBCA Forecast Dashboard",
    page_icon="üìà",
    layout="wide",
)


# ====== UI THEME / ACCENT ======
with st.sidebar:
    st.markdown("### üé® Appearance")
    accent = st.color_picker("Accent color", "#22d3ee")  # ganti default sesuka lo

st.markdown(
    f"""
<style>
:root {{
  --accent: {accent};
  --bg: #0b1220;
  --panel: rgba(255,255,255,0.06);
  --panel2: rgba(0,0,0,0.25);
  --border: rgba(255,255,255,0.10);
  --text: #e5e7eb;
  --muted: rgba(229,231,235,0.70);
}}

/* Make main container truly wide */
.block-container {{
  max-width: 100% !important;
  padding-top: 1.2rem !important;
  padding-left: 1.25rem !important;
  padding-right: 1.25rem !important;
}}

/* Hide Streamlit top header + toolbar (yang ada Deploy/‚Ä¶ itu) */
header[data-testid="stHeader"] {{
  display: none !important; 
}}
div[data-testid="stToolbar"] {{ 
  display: none !important; 
}}

/* (opsional) ilangin padding top yang biasanya disisain header */
.block-container {{
  padding-top: 0.6rem !important;
}}


/* Background + glow */
.stApp {{
  background: radial-gradient(800px 500px at 15% 10%, color-mix(in srgb, var(--accent) 35%, transparent), transparent 60%),
              radial-gradient(700px 450px at 85% 5%, rgba(16,185,129,0.18), transparent 55%),
              var(--bg);
}}

[data-testid="stSidebar"] > div {{
  background: rgba(255,255,255,0.04);
  border-right: 1px solid var(--border);
}}

/* Cards look */
div[data-testid="stMetric"] {{
  background: var(--panel);
  border: 1px solid var(--border);
  border-radius: 16px;
  padding: 14px 14px;
}}

div[data-testid="stMetric"] * {{
  color: var(--text) !important;
}}

div[data-testid="stMetric"] label {{
  color: var(--muted) !important;
}}

/* Tabs */
/* =======================
   Tabs (Overview / Forecast / Explain / Raw Data)
   ======================= */

.stTabs [data-baseweb="tab-list"] {{
  gap: 8px;
}}

/* Default tab */
.stTabs [data-baseweb="tab"] {{
  background: rgba(255,255,255,0.04);
  border: 1px solid var(--border);
  border-radius: 999px;
  padding: 10px 16px;
  color: var(--muted);
  transition: all 0.18s ease;
}}

/* Hover: cuma terangin border + text dikit */
.stTabs [data-baseweb="tab"]:hover {{
  border-color: rgba(255,255,255,0.35);
  color: var(--text);
  background: rgba(255,255,255,0.06);
}}

/* Selected tab: baru pakai accent */
.stTabs [aria-selected="true"] {{
  color: var(--text);
  background: color-mix(in srgb, var(--accent) 22%, rgba(255,255,255,0.06));
  border-color: color-mix(in srgb, var(--accent) 55%, var(--border));
  box-shadow: 0 0 0 1px color-mix(in srgb, var(--accent) 45%, transparent);
}}

/* Selected + hover (biar nggak lompat warna) */
.stTabs [aria-selected="true"]:hover {{
  background: color-mix(in srgb, var(--accent) 26%, rgba(255,255,255,0.08));
}}



/* Buttons */
.stButton > button, .stDownloadButton > button {{
  border-radius: 14px !important;
  border: 1px solid color-mix(in srgb, var(--accent) 55%, var(--border)) !important;
  background: color-mix(in srgb, var(--accent) 22%, rgba(255,255,255,0.06)) !important;
}}
.stButton > button:hover, .stDownloadButton > button:hover {{
  filter: brightness(1.08);
}}

/* Dataframe */
div[data-testid="stDataFrame"] {{
  border-radius: 16px;
  overflow: hidden;
  border: 1px solid var(--border);
}}
</style>
""",
    unsafe_allow_html=True,
)


# =========================
# FILE PATHS
# =========================
CKPT_PATH = "tft_model.ckpt"
PARAMS_PATH = "tft_dataset_params.pkl"
FUND_PATH = "bbca_fundamentals_quarterly_2021_2023.csv"

TICKER = "BBCA.JK"


PROTECTED = {
    # time index internals
    "relative_time_idx",
    "time_idx_start",
    "encoder_length",
    "decoder_length",

    # target scaling / centering internals (sering muncul)
    "target_scale",
    "target_center",
    "center",
    "scale",

    # kalau target kamu ret_log, ini yang muncul:
    "ret_log_center",
    "ret_log_scale",

    # kalau targetnya Close, kadang juga ada:
    "Close_center",
    "Close_scale",
}
def drop_protected_cols(df: pd.DataFrame) -> pd.DataFrame:
    cols = [c for c in df.columns if c in PROTECTED]
    return df.drop(columns=cols, errors="ignore")

# =========================
# Yahoo realtime (no yfinance)
# =========================
@st.cache_data(ttl=60 * 15)
def load_bbca_price_from_yahoo(period="5y", interval="1d") -> pd.DataFrame:
    url = f"https://query1.finance.yahoo.com/v8/finance/chart/{TICKER}"
    params = {"range": period, "interval": interval, "includePrePost": "false"}
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json,text/plain,*/*",
    }
    r = requests.get(url, params=params, headers=headers, timeout=20)
    r.raise_for_status()
    j = r.json()

    result = j.get("chart", {}).get("result")
    if not result:
        raise ValueError(f"Yahoo chart kosong: {j.get('chart', {}).get('error')}")

    res0 = result[0]
    ts = res0.get("timestamp", [])
    q = res0.get("indicators", {}).get("quote", [{}])[0]

    df = pd.DataFrame({
        "Date": pd.to_datetime(ts, unit="s").tz_localize(None),
        "Open": q.get("open"),
        "High": q.get("high"),
        "Low": q.get("low"),
        "Close": q.get("close"),
        "Volume": q.get("volume"),
    }).dropna()

    df = df.sort_values("Date").groupby("Date", as_index=False).last()
    return df


# =========================
# Feature engineering
# =========================
def rsi(series: pd.Series, period: int = 14) -> pd.Series:
    delta = series.diff()
    up = delta.clip(lower=0.0)
    down = (-delta).clip(lower=0.0)
    roll_up = up.ewm(alpha=1/period, adjust=False).mean()
    roll_down = down.ewm(alpha=1/period, adjust=False).mean()
    rs = roll_up / (roll_down + 1e-12)
    return 100.0 - (100.0 / (1.0 + rs))

def macd(series: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9):
    ema_fast = series.ewm(span=fast, adjust=False).mean()
    ema_slow = series.ewm(span=slow, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    macd_signal = macd_line.ewm(span=signal, adjust=False).mean()
    macd_hist = macd_line - macd_signal
    return macd_line, macd_signal, macd_hist

def add_calendar_feats(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    out["dayofweek"] = out["Date"].dt.dayofweek
    out["month"] = out["Date"].dt.month
    out["day_sin"] = np.sin(2*np.pi*out["dayofweek"]/7)
    out["day_cos"] = np.cos(2*np.pi*out["dayofweek"]/7)
    out["mon_sin"] = np.sin(2*np.pi*(out["month"]-1)/12)
    out["mon_cos"] = np.cos(2*np.pi*(out["month"]-1)/12)
    return out

def add_technicals(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    out = out.loc[:, ~out.columns.duplicated()].copy()

    # out["ret_log"] = np.log(out["Close"]).diff()
    out["ret_log"] = np.log(out["Close"]).diff()

    out["ret_log_cum_5"] = out["ret_log"].rolling(5, min_periods=5).sum()
    out["ret_log_sma_5"] = out["ret_log"].rolling(5, min_periods=5).mean()
    out["ret_log_ema_5"] = out["ret_log"].ewm(span=5, adjust=False).mean()
    out["ret_log_ema_10"] = out["ret_log"].ewm(span=10, adjust=False).mean()

    out["SMA_20"] = out["Close"].rolling(20, min_periods=20).mean()
    out["EMA_20"] = out["Close"].ewm(span=20, adjust=False).mean()

    w = 20
    m = out["Close"].rolling(w, min_periods=w).mean()
    s = out["Close"].rolling(w, min_periods=w).std()
    out["Bollinger_upper"] = m + 2*s
    out["Bollinger_lower"] = m - 2*s

    out["BB_percentB"] = (out["Close"] - out["Bollinger_lower"]) / (out["Bollinger_upper"] - out["Bollinger_lower"] + 1e-12)
    out["BB_bandwidth"] = (out["Bollinger_upper"] - out["Bollinger_lower"]) / (m + 1e-12)

    out["RSI_14"] = rsi(out["Close"], 14)
    out["MACD_line"], out["MACD_signal"], out["MACD_hist"] = macd(out["Close"])

    out["roll_std_5"] = out["ret_log"].rolling(5, min_periods=5).std()
    out["roll_std_10"] = out["ret_log"].rolling(10, min_periods=10).std()
    return out

def add_lags(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    lag_cols = [
        "Close","Volume","SMA_20","EMA_20","BB_percentB","BB_bandwidth",
        "RSI_14","MACD_line","MACD_signal","MACD_hist",
        "ret_log","roll_std_5","roll_std_10",
        "Bollinger_upper","Bollinger_lower",
    ]
    for c in lag_cols:
        if c in out.columns:
            out[c+"_lag1"] = out[c].shift(1)
            out[c+"_lag5"] = out[c].shift(5)
    return out


# =========================
# Fundamentals: quarterly -> daily + QoQ/YoY
# =========================
def quarter_end_date(q_label: str) -> str:
    q_label = (q_label or "").upper().strip()
    mapping = {"Q1":"03-31","Q2":"06-30","Q3":"09-30","Q4":"12-31"}
    return mapping.get(q_label, "12-31")

def _to_num(x):
    if pd.isna(x):
        return np.nan
    s = str(x).strip().replace(",", "").replace("%", "")
    try:
        return float(s)
    except:
        return np.nan

def load_fundamentals_quarterly(path: str) -> pd.DataFrame:
    f = pd.read_csv(path)
    f.columns = [c.strip() for c in f.columns]
    if "Periode" not in f.columns or "Quartal" not in f.columns:
        raise ValueError("Fund CSV harus punya kolom 'Periode' dan 'Quartal'.")

    years = f["Periode"].astype(str).str.extract(r"(\d{4})")[0]
    qend = f["Quartal"].astype(str).map(quarter_end_date)
    f["Date"] = pd.to_datetime(years + "-" + qend, errors="coerce")

    num_cols = [c for c in f.columns if c not in ["Periode","Quartal","Date"]]
    for c in num_cols:
        f[c] = f[c].map(_to_num)

    f = f.sort_values("Date").reset_index(drop=True)
    f["Periode_QoQ"] = np.arange(len(f), dtype=float)          # 0,1,2,... per quarter
    f["Periode_YoY"] = f["Periode_QoQ"] / 4.0

    for c in num_cols:
        f[c + "_QoQ"] = f[c].pct_change(1)
        f[c + "_YoY"] = f[c].pct_change(4)
    base = ["Date", "Periode_QoQ", "Periode_YoY"]
    return f[base + num_cols + [c+"_QoQ" for c in num_cols] + [c+"_YoY" for c in num_cols]]
    # return f[["Date"] + num_cols + [c+"_QoQ" for c in num_cols] + [c+"_YoY" for c in num_cols]]

def fundamentals_to_daily(fq: pd.DataFrame, dates: pd.Series) -> pd.DataFrame:
    daily = fq.set_index("Date").sort_index().ffill().bfill()
    daily = daily.reindex(pd.to_datetime(dates)).ffill().bfill()
    daily = daily.reset_index().rename(columns={"index":"Date"})
    return daily


# =========================
# Build df_tft + align to params
# =========================
def build_df_tft(price: pd.DataFrame, fund_daily: pd.DataFrame) -> pd.DataFrame:
    df = price.copy()
    df = add_calendar_feats(df)
    df = add_technicals(df)
    df = add_lags(df)

    # shift fundamentals 7 hari (sesuai training kamu)
    # fund_shift = fund_daily.copy()
    # shift_cols = [c for c in fund_shift.columns if c != "Date"]
    # fund_shift[shift_cols] = fund_shift[shift_cols].shift(7)
    # df = df.merge(fund_shift, on="Date", how="left")
    # fund_shift = fund_daily.copy()

    # # pastikan numeric dan terisi
    # fund_cols = [c for c in fund_shift.columns if c != "Date"]
    # for c in fund_cols:
    #     fund_shift[c] = pd.to_numeric(fund_shift[c], errors="coerce")
    # fund_shift[fund_cols] = fund_shift[fund_cols].replace([np.inf, -np.inf], np.nan).ffill().bfill()

    # # ‚úÖ shift 7 HARI (bukan shift 7 baris)
    # fund_shift["Date"] = pd.to_datetime(fund_shift["Date"]) + pd.Timedelta(days=7)

    # df = df.merge(fund_shift, on="Date", how="left")
    # df[fund_cols] = df[fund_cols].ffill().bfill()

    df= df.merge(fund_daily, on="Date", how="left")

    df = df.loc[:, ~df.columns.duplicated()].copy()

    # group_id fixed
    df["group_id"] = "BBCA"
    df["group_id"] = df["group_id"].astype(str)

    # time_idx
    df = df.sort_values("Date").reset_index(drop=True)
    df["time_idx"] = np.arange(len(df), dtype=np.int64)

    return df


def align_df_to_dataset_params(df: pd.DataFrame, params: dict) -> pd.DataFrame:
    """
    Pastikan semua kolom yang dibutuhkan params ada dan tidak NA/inf.
    Kalau ada yang missing, dibuat dan diisi 0 lalu ffill/bfill.
    """
    # kolom-kolom yang biasanya ada di params
    need_cols = set()
    for k in [
        "time_varying_known_reals",
        "time_varying_unknown_reals",
        "static_reals",
        "static_categoricals",
        "time_varying_known_categoricals",
        "time_varying_unknown_categoricals",
    ]:
        v = params.get(k, None)
        if isinstance(v, (list, tuple)):
            need_cols |= set(v)

    # selalu butuh ini
    need_cols |= {"time_idx", "group_id"}

    # buat kolom yang belum ada
    for c in need_cols:
        if c not in df.columns:
            df[c] = np.nan

    # casting numeric untuk reals
    reals = set()
    for k in ["time_varying_known_reals", "time_varying_unknown_reals", "static_reals"]:
        v = params.get(k, None)
        if isinstance(v, (list, tuple)):
            reals |= set(v)

    for c in reals:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # handle categoricals
    cats = set()
    for k in ["static_categoricals", "time_varying_known_categoricals", "time_varying_unknown_categoricals"]:
        v = params.get(k, None)
        if isinstance(v, (list, tuple)):
            cats |= set(v)
    for c in cats:
        if c in df.columns:
            df[c] = df[c].astype(str)

    # replace inf -> nan
    df = df.replace([np.inf, -np.inf], np.nan)

    # fill: time-series fill untuk semua columns needed (kecuali Date)
    fill_cols = [c for c in need_cols if c in df.columns and c != "Date"]
    df[fill_cols] = df[fill_cols].ffill().bfill()

    return df


def make_future_rows(df_hist: pd.DataFrame, horizon: int) -> pd.DataFrame:
    """
    Tambah row future (business days) untuk known features.
    Unknowns akan di-fill oleh aligner (ffill) tapi untuk beberapa (Close/ret_log) akan kita set saat recursive.
    """
    df = df_hist.copy().reset_index(drop=True)
    last_date = pd.to_datetime(df["Date"].iloc[-1])
    future_dates = pd.bdate_range(last_date + pd.Timedelta(days=1), periods=horizon)

    last_time = int(df["time_idx"].iloc[-1])
    fut = pd.DataFrame({"Date": pd.to_datetime(future_dates)})
    fut["time_idx"] = np.arange(last_time + 1, last_time + 1 + horizon, dtype=np.int64)
    fut["group_id"] = "BBCA"

    fut = add_calendar_feats(fut)  # known calendar feats

    df_all = pd.concat([df, fut], ignore_index=True, sort=False)
    return df_all


# =========================
# Prediction
# =========================

@st.cache_resource
def load_model_and_params():
    if not os.path.exists(CKPT_PATH):
        raise FileNotFoundError(f"CKPT tidak ketemu: {CKPT_PATH}")
    if not os.path.exists(PARAMS_PATH):
        raise FileNotFoundError(f"Params tidak ketemu: {PARAMS_PATH}")

    # --- PATCH: allow loading pickles created with older pandas internals ---
    if "pandas.core.indexes.numeric" not in sys.modules:
        numeric_mod = types.ModuleType("pandas.core.indexes.numeric")
    
        # Old pandas used these names; map them to current Index implementations.
        # This is enough for most pickled artifacts that contain Index objects.
        numeric_mod.NumericIndex = pd.Index
        numeric_mod.Int64Index = pd.Index
        numeric_mod.UInt64Index = pd.Index
        numeric_mod.Float64Index = pd.Index
    
        sys.modules["pandas.core.indexes.numeric"] = numeric_mod
    # -----------------------------------------------------------------------
    
    with open(PARAMS_PATH, "rb") as f:
        params = pickle.load(f)
        # --- PATCH: compat for pickled GroupNormalizer across pytorch-forecasting versions ---
        tn = params.get("target_normalizer", None)
        
        # target_normalizer kadang ke-pickle sebagai instance GroupNormalizer
        if isinstance(tn, GroupNormalizer) and not hasattr(tn, "_groups"):
            # coba ambil dari atribut yang mungkin ada di versi lain
            if hasattr(tn, "groups") and tn.groups is not None:
                try:
                    tn._groups = list(tn.groups)
                except Exception:
                    tn._groups = [tn.groups]
            elif hasattr(tn, "group_ids") and tn.group_ids is not None:
                try:
                    tn._groups = list(tn.group_ids)
                except Exception:
                    tn._groups = [tn.group_ids]
            else:
                # fallback minimal: group_id kamu memang satu
                tn._groups = ["group_id"]
        
            params["target_normalizer"] = tn
        # -------------------------------------------------------------------------------


    # ‚úÖ bypass PyTorch 2.6 weights_only default
    ckpt = torch.load(CKPT_PATH, map_location="cpu", weights_only=False)

    # lightning ckpt biasanya dict dengan keys: "state_dict", "hyper_parameters"
    hparams = ckpt.get("hyper_parameters", {})
    state_dict = ckpt["state_dict"]

    # build model dari hparams yg disimpan di ckpt, lalu load weights
    model = TemporalFusionTransformer(**hparams)
    model.load_state_dict(state_dict, strict=True)
    model.eval()

    return model, params


def predict_direct(model, params, df_all: pd.DataFrame) -> np.ndarray:
    """
    Direct predict sesuai max_prediction_length yang ada di ckpt.
    Return array shape (horizon,)
    """
    df_all = drop_protected_cols(df_all)
    ds = TimeSeriesDataSet.from_parameters(params, df_all, predict=True, stop_randomization=True)
    loader = ds.to_dataloader(train=False, batch_size=64, num_workers=0)
    with torch.no_grad():
        pred = model.predict(loader)  # (N, pred_len)
    arr = pred.detach().cpu().numpy()
    return arr[-1]  # last window


def recursive_forecast(model, params, df_hist: pd.DataFrame, horizon: int) -> pd.DataFrame:
    """
    Untuk model H=1: predict 1 langkah, update Close dengan exp(ret_pred), ulangi.
    """
    df = df_hist.copy().reset_index(drop=True)

    # pastikan ada kolom inti
    if "Close" not in df.columns or "ret_log" not in df.columns:
        raise ValueError("df_hist wajib punya kolom Close dan ret_log")

    last_close = float(df["Close"].iloc[-1])

    rows = []
    future_dates = pd.bdate_range(pd.to_datetime(df["Date"].iloc[-1]) + pd.Timedelta(days=1), periods=horizon)

    for d in future_dates:
        # tambah 1 row future
        next_time = int(df["time_idx"].iloc[-1]) + 1
        new_row = {"Date": pd.to_datetime(d), "time_idx": next_time, "group_id": "BBCA"}
        new_row = pd.DataFrame([new_row])
        new_row = add_calendar_feats(new_row)

        df = pd.concat([df, new_row], ignore_index=True, sort=False)

        # align columns & fill knowns
        df_aligned = align_df_to_dataset_params(df.copy(), params)

        # predict 1-step
        ret_pred_arr = predict_direct(model, params, df_aligned)
        ret_pred = float(ret_pred_arr[0])  # H=1

        close_pred = last_close * float(np.exp(ret_pred))

        # write back to df for next loop
        df.loc[df.index[-1], "ret_log"] = ret_pred
        df.loc[df.index[-1], "Close"] = close_pred

        rows.append({"Date": pd.to_datetime(d), "ret_pred": ret_pred, "Close_pred": close_pred})
        last_close = close_pred

    return pd.DataFrame(rows)

def add_profit_columns(pred_df: pd.DataFrame, anchor_close: float) -> pd.DataFrame:
    out = pred_df.copy()

    # kalau belum ada kolom numerik % nya, bikin dulu
    if "% Untung/Rugi dari Historical Close_num" not in out.columns:
        out["% Untung/Rugi dari Historical Close_num"] = (out["Close_pred"] / anchor_close - 1.0) * 100.0

    if "% Naik/Turun vs Hari Sebelumnya_num" not in out.columns:
        out["% Naik/Turun vs Hari Sebelumnya_num"] = out["Close_pred"].pct_change() * 100.0
        if len(out) > 0:
            out.loc[out.index[0], "% Naik/Turun vs Hari Sebelumnya_num"] = out["% Untung/Rugi dari Historical Close_num"].iloc[0]

    # kolom display string
    def fmt_pct(x):
        if pd.isna(x):
            return ""
        return f"{x:+.4f}%"

    out["% Untung/Rugi dari Historical Close"] = out["% Untung/Rugi dari Historical Close_num"].apply(fmt_pct)
    out["% Naik/Turun vs Hari Sebelumnya"] = out["% Naik/Turun vs Hari Sebelumnya_num"].apply(fmt_pct)

    return out

def get_feature_importance_table(
    model,
    params,
    df_for_interp: pd.DataFrame,
    top_k: int = 25,
    n_batches: int = 5,
) -> pd.DataFrame:
    """
    Ambil feature importance dari TFT lewat variable selection weights langsung dari forward().
    Lebih kompatibel antar versi pytorch-forecasting dibanding interpret_output.
    """
    df_for_interp = drop_protected_cols(df_for_interp)

    ds = TimeSeriesDataSet.from_parameters(params, df_for_interp, predict=True, stop_randomization=True)
    loader = ds.to_dataloader(train=False, batch_size=64, num_workers=0)

    # nama feature yang dipakai model (biasanya tersedia)
    enc_names = getattr(model, "encoder_variables", None)
    dec_names = getattr(model, "decoder_variables", None)

    enc_sum = None
    dec_sum = None
    enc_count = 0
    dec_count = 0

    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        for i, batch in enumerate(loader):
            if i >= n_batches:
                break

            # dataloader biasanya return (x, y) atau dict
            if isinstance(batch, (list, tuple)) and len(batch) >= 1:
                x = batch[0]
            else:
                x = batch

            # pindah ke device
            if isinstance(x, dict):
                x = {k: (v.to(device) if torch.is_tensor(v) else v) for k, v in x.items()}
            else:
                # fallback kalau format aneh
                continue

            out = model(x)

            # beberapa versi pakai key berbeda
            enc_w = out.get("encoder_variable_selection", None) or out.get("encoder_variables", None)
            dec_w = out.get("decoder_variable_selection", None) or out.get("decoder_variables", None)

            # enc_w/dec_w biasanya shape: [B, T, F] atau [B, F]
            if enc_w is not None and torch.is_tensor(enc_w):
                w = enc_w
                while w.dim() > 2:
                    w = w.mean(dim=1)  # rata-rata time
                w = w.mean(dim=0)      # rata-rata batch -> [F]
                enc_sum = w if enc_sum is None else enc_sum + w
                enc_count += 1

            if dec_w is not None and torch.is_tensor(dec_w):
                w = dec_w
                while w.dim() > 2:
                    w = w.mean(dim=1)
                w = w.mean(dim=0)
                dec_sum = w if dec_sum is None else dec_sum + w
                dec_count += 1

    if enc_sum is None and dec_sum is None:
        raise ValueError("Tidak ketemu variable selection weights di output model. Coba cek keys output forward model.")

    rows = []

    if enc_sum is not None and enc_count > 0:
        enc_avg = (enc_sum / enc_count).detach().cpu().numpy()
        if enc_names is None:
            enc_names = [f"enc_{i}" for i in range(len(enc_avg))]
        for n, v in zip(enc_names, enc_avg):
            rows.append((n, float(v), "encoder"))

    if dec_sum is not None and dec_count > 0:
        dec_avg = (dec_sum / dec_count).detach().cpu().numpy()
        if dec_names is None:
            dec_names = [f"dec_{i}" for i in range(len(dec_avg))]
        for n, v in zip(dec_names, dec_avg):
            rows.append((n, float(v), "decoder"))

    imp = pd.DataFrame(rows, columns=["feature", "importance", "part"])

    # gabung encoder+decoder kalau nama sama
    imp = (
        imp.groupby("feature", as_index=False)["importance"]
           .mean()
           .sort_values("importance", ascending=False)
           .head(top_k)
           .reset_index(drop=True)
    )

    # normalize biar gampang dibaca (optional)
    s = imp["importance"].sum()
    if s > 0:
        imp["importance"] = imp["importance"] / s

    return imp


# =========================
# Temporal importance (TFT attention via interpret_output)
# =========================
def get_temporal_importance_table_interpret_output(
    model,
    params,
    df_for_interp: pd.DataFrame,
) -> pd.DataFrame:
    """
    Temporal importance dari attention decoder->encoder via interpret_output.

    Penting:
      - Kalau attention tidak ditemukan / tidak terbaca, fungsi ini akan RAISE error.
        (Supaya tidak jatuh ke nilai uniform yang bikin semua persen sama.)
    """
    df_for_interp = drop_protected_cols(df_for_interp.copy())
    ds = TimeSeriesDataSet.from_parameters(params, df_for_interp, predict=True, stop_randomization=True)
    loader = ds.to_dataloader(train=False, batch_size=64, num_workers=0)

    model.eval()
    with torch.no_grad():
        pred = model.predict(loader, mode="raw", return_x=True)

    # Ambil raw output (beda versi bisa beda bentuk)
    if hasattr(pred, "output"):
        raw = pred.output
    elif isinstance(pred, (tuple, list)) and len(pred) > 0:
        raw = pred[0]
    else:
        raw = pred

    # interpret
    interpretation = model.interpret_output(raw, reduction="sum")
    att = interpretation.get("attention", None)
    if att is None:
        raise RuntimeError("interpret_output tidak mengembalikan 'attention'. Cek versi pytorch-forecasting.")

    # extract tensor
    att_tensor = None
    if torch.is_tensor(att):
        att_tensor = att
    elif isinstance(att, dict):
        # cari tensor pertama di dict
        for v in att.values():
            if torch.is_tensor(v):
                att_tensor = v
                break

    if att_tensor is None:
        raise RuntimeError("Attention ditemukan tapi formatnya tidak terbaca (bukan tensor).")

    a = att_tensor.detach().float().cpu()

    # biasanya [B, dec_len, enc_len] -> reduce ke [enc_len]
    if a.ndim == 3:
        enc_scores = a.mean(dim=0).mean(dim=0)  # [enc]
        enc_scores = enc_scores.numpy()
    elif a.ndim == 2:
        enc_scores = a.mean(dim=0)              # [enc]
        enc_scores = enc_scores.numpy()
    elif a.ndim == 1:
        # Beberapa versi mengembalikan attention yang SUDAH diringkas ke encoder_len: shape (enc_len,)
        enc_scores = a.numpy()
    else:
        raise RuntimeError(f"Shape attention tidak didukung: {tuple(a.shape)}")

    # normalize
    s = float(np.sum(enc_scores))
    if s <= 0:
        raise RuntimeError("Attention sum <= 0 (aneh).")
    enc_imp = enc_scores / s

    # map ke tanggal (encoder window terakhir)
    enc_len = int(params.get("max_encoder_length", len(enc_imp)))
    enc_len = min(enc_len, len(df_for_interp), len(enc_imp))

    dates = pd.to_datetime(df_for_interp["Date"].iloc[-enc_len:]).reset_index(drop=True)
    scores = enc_imp[-enc_len:]

    temp = pd.DataFrame({"Date": dates, "Temporal Importance": scores}).sort_values("Date").reset_index(drop=True)
    temp["Temporal Importance (%)"] = (temp["Temporal Importance"] * 100).round(4)
    return temp



# Optional Plotly
try:
    import plotly.graph_objects as go
    _HAS_PLOTLY = True
except Exception:
    _HAS_PLOTLY = False

# ===== session state init =====
for k, v in {
    "pred_df": None,
    "anchor_date": None,
    "anchor_close": None,
    "feature_importance": None,
    "temporal_importance": None,
}.items():
    if k not in st.session_state:
        st.session_state[k] = v

# ---------- Load model + params ----------
try:
    model, params = load_model_and_params()
except Exception as e:
    st.error("Gagal load ckpt/params.")
    st.exception(e)
    st.stop()

# ---------- Sidebar Controls (form to reduce reruns) ----------
with st.sidebar:
    st.markdown("### ‚öôÔ∏è Controls")

    with st.form("controls_form", border=False):
        load_period = st.selectbox("Load history (untuk model)", ["2y", "5y", "10y"], index=2)

        today = pd.Timestamp.today().normalize()
        default_view_start = (today - pd.DateOffset(years=1)).date()
        default_view_end = today.date()

        view_range = st.date_input("View range (yang ditampilkan)", value=(default_view_start, default_view_end))
        forecast_from_view_end = st.checkbox("Forecast mulai dari end view range", value=True)
        horizon = st.slider("Forecast horizon (trading days)", 1, 20, 20)

        run = st.form_submit_button("üöÄ Run Forecast", type="primary")

    st.caption(f"Fundamentals file: {FUND_PATH}")

# ---------- Load price ----------
try:
    price = load_bbca_price_from_yahoo(period=load_period, interval="1d")
except Exception as e:
    st.error("Gagal ambil BBCA.JK dari Yahoo endpoint.")
    st.exception(e)
    st.stop()

# ---------- Load fundamentals (keep EXACT logic from original app.py) ----------
if not os.path.exists(FUND_PATH):
    st.error(f"Fundamentals CSV tidak ketemu: {FUND_PATH}")
    st.stop()

fund_q = load_fundamentals_quarterly(FUND_PATH)

# shift availability 7 hari (date + 7d)
fund_q = fund_q.sort_values("Date").copy()
fund_q["Date"] = pd.to_datetime(fund_q["Date"]) + pd.Timedelta(days=7)

# align ke trading dates (price dates) pake ffill
price_dates = pd.to_datetime(price["Date"]).sort_values()
fund_daily = (
    fund_q.set_index("Date")
          .sort_index()
          .reindex(price_dates, method="ffill")
          .bfill()
          .reset_index()
          .rename(columns={"index": "Date"})
)

# ---------- Build df_tft + align ----------
df = build_df_tft(price, fund_daily)
df = align_df_to_dataset_params(df, params)

# ---------- View range parsing ----------
if isinstance(view_range, tuple) and len(view_range) == 2:
    view_start = pd.to_datetime(view_range[0])
    view_end = pd.to_datetime(view_range[1])
else:
    view_start = pd.to_datetime(default_view_start)
    view_end = pd.to_datetime(default_view_end)

min_d = df["Date"].min()
max_d = df["Date"].max()
vs = max(view_start, min_d)
ve = min(view_end, max_d)

df_view = df[(df["Date"] >= vs) & (df["Date"] <= ve)].copy()
if df_view.empty:
    st.warning("View range kosong (tidak ada data di range itu). Coba geser tanggalnya.")

# ----- choose anchor dataset -----
if forecast_from_view_end:
    df_model = df[df["Date"] <= ve].copy()
    if df_model.empty:
        df_model = df.copy()
else:
    df_model = df.copy()

anchor_date = pd.to_datetime(df_model["Date"].iloc[-1]).date()
anchor_close = float(df_model["Close"].iloc[-1])

st.markdown("""
<div style="
  padding: 14px 16px;
  border: 1px solid rgba(255,255,255,0.10);
  background: rgba(255,255,255,0.05);
  border-radius: 18px;
">
  <div style="font-size:12px; letter-spacing:0.18em; text-transform:uppercase; color:rgba(229,231,235,0.65);">
    Temporal Fusion Transformer ‚Ä¢ Forecast Dashboard
  </div>
  <div style="font-size:32px; font-weight:800; margin-top:6px;">
    BBCA Forecast Dashboard
  </div>
  <div style="font-size:14px; color:rgba(229,231,235,0.75); margin-top:6px;">
    Forecast & explainability with real TFT model (BBCA.JK).
  </div>
</div>
""", unsafe_allow_html=True)

st.write("")  # spacer


# ---------- Metrics row ----------
m1, m2, m3, m4 = st.columns(4)
m1.metric("Anchor Date", str(anchor_date))
m2.metric("Anchor Close", f"{anchor_close:,.0f}")
m3.metric("Horizon", f"{horizon} hari")
m4.metric("History Loaded", load_period)

# ---------- Tabs ----------
tab_overview, tab_forecast, tab_explain, tab_data = st.tabs(
    ["üìä Overview", "üîÆ Forecast", "üß† Explainability", "üßæ Raw Data"]
)

# ---------- Overview ----------
with tab_overview:
    c1, c2 = st.columns([1.35, 1])

    with c1:
        st.subheader("Historical Close")
        if _HAS_PLOTLY:
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=df_view["Date"], y=df_view["Close"], mode="lines", name="Close"))
            fig.update_layout(height=420, margin=dict(l=10, r=10, t=30, b=10), xaxis_title="Date", yaxis_title="Close")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.line_chart(df_view.set_index("Date")["Close"], height=420)

    with c2:
        st.subheader("Latest rows (view)")
        st.dataframe(df_view.tail(12), use_container_width=True, hide_index=True)

    with st.expander("üîß Debug / Data Quality", expanded=False):
        if "CAR/KPMM (%)" in fund_daily.columns:
            st.write("CAR NA ratio (fund_daily):", float(fund_daily["CAR/KPMM (%)"].isna().mean()))
        else:
            st.info("Kolom 'CAR/KPMM (%)' tidak ditemukan di fund_daily.")

# ---------- Run Forecast action ----------
if run:
    pred_len = params.get("max_prediction_length", 1)
    try:
        pred_len = int(pred_len)
    except Exception:
        pred_len = 1

    with st.spinner("Running model inference‚Ä¶"):
        try:
            if pred_len >= horizon:
                df_all = make_future_rows(df_model, horizon=horizon)
                df_all = align_df_to_dataset_params(df_all, params)

                ret_pred = predict_direct(model, params, df_all)[:horizon]
                last_close = float(df_model["Close"].iloc[-1])

                close_path = []
                cur = last_close
                for r in ret_pred:
                    cur = cur * float(np.exp(float(r)))
                    close_path.append(cur)

                future_dates = pd.bdate_range(
                    pd.to_datetime(df_model["Date"].iloc[-1]) + pd.Timedelta(days=1),
                    periods=horizon
                )
                pred_df = pd.DataFrame({"Date": future_dates, "ret_pred": ret_pred, "Close_pred": close_path})
            else:
                pred_df = recursive_forecast(model, params, df_model, horizon=horizon)
        except Exception as e:
            st.error("Forecast gagal.")
            st.exception(e)
            st.stop()

    # keep your exact profit formatting behavior
    pred_df = add_profit_columns(pred_df, anchor_close)

    st.session_state.pred_df = pred_df.copy()
    st.session_state.anchor_date = anchor_date
    st.session_state.anchor_close = anchor_close
    st.session_state.feature_importance = None
    st.session_state.temporal_importance = None

    st.success("Forecast selesai! Lihat tab Forecast & Explainability.")
    st.rerun()

# ---------- Forecast tab ----------
with tab_forecast:
    if st.session_state.pred_df is None:
        st.info("Atur parameter di sidebar lalu klik **Run Forecast**.")
    else:
        pred_df = st.session_state.pred_df.copy()

        st.subheader("Forecast Close (future)")
        if _HAS_PLOTLY:
            fig2 = go.Figure()
            fig2.add_trace(go.Scatter(x=pred_df["Date"], y=pred_df["Close_pred"], mode="lines+markers", name="Close_pred"))
            fig2.update_layout(height=420, margin=dict(l=10, r=10, t=30, b=10), xaxis_title="Date", yaxis_title="Close_pred")
            st.plotly_chart(fig2, use_container_width=True)
        else:
            st.line_chart(pred_df.set_index("Date")["Close_pred"], height=420)

        st.subheader("Forecast table")
        show = pred_df[["Date", "Close_pred", "% Untung/Rugi dari Historical Close", "% Naik/Turun vs Hari Sebelumnya"]].copy()
        show = show.reset_index(drop=True)
        show.index = show.index + 1
        show.index.name = "No"

        def color_pct(val):
            if not isinstance(val, str) or val == "":
                return ""
            if val.startswith("+"):
                return "color: #16a34a; font-weight: 700;"
            if val.startswith("-"):
                return "color: #dc2626; font-weight: 700;"
            return ""

        styled = (
            show.style
            .format({"Close_pred": "{:,.0f}"})
            .applymap(color_pct, subset=["% Untung/Rugi dari Historical Close", "% Naik/Turun vs Hari Sebelumnya"])
        )
        st.dataframe(styled, use_container_width=True)

        st.download_button(
            "‚¨áÔ∏è Download forecast CSV",
            data=pred_df.to_csv(index=False).encode("utf-8"),
            file_name=f"BBCA_forecast_{horizon}d.csv",
            mime="text/csv",
            key="download_forecast",
        )

# ---------- Explainability tab ----------
with tab_explain:
    if st.session_state.pred_df is None:
        st.info("Jalankan forecast dulu supaya explainability relevan dengan anchor terbaru.")
    else:
        sub_var, sub_time = st.tabs(["üß† Variable Importance", "‚è≥ Temporal Importance (Attention)"])

        with sub_var:
            st.caption("Variable importance dihitung dari **variable selection weights** (lebih kompatibel).")
            top_k = st.slider("Top features", 5, 50, 20, key="top_k_slider")

            if st.session_state.feature_importance is None:
                with st.spinner("Menghitung feature importance‚Ä¶"):
                    try:
                        df_interp = df_model.tail(400).copy()
                        df_interp = align_df_to_dataset_params(df_interp, params)
                        fi = get_feature_importance_table(model, params, df_interp, top_k=50, n_batches=5)
                        fi["Persentase Perhatian Model (%)"] = (fi["importance"] * 100).round(2)
                        fi = fi.drop(columns=["importance"])
                        st.session_state.feature_importance = fi.copy()
                    except Exception as e:
                        st.error("Gagal ambil feature importance.")
                        st.exception(e)

            if st.session_state.feature_importance is not None:
                fi_full = st.session_state.feature_importance.copy()
                fi_show = fi_full.head(top_k).reset_index(drop=True)
                fi_show.index = fi_show.index + 1
                fi_show.index.name = "No"
                st.dataframe(fi_show, use_container_width=True)

        with sub_time:
            st.caption(
                "Temporal importance berbasis **attention decoder‚Üíencoder** (fungsi TFT). "
                "Ini menunjukkan tanggal-tanggal historis yang paling diperhatikan model saat membuat prediksi."
            )

            # ‚úÖ Slider yang boleh ada: TOP DATES
            max_enc = int(params.get("max_encoder_length", 90) or 90)
            top_dates = st.slider(
                "Top-N tanggal paling crucial",
                min_value=5,
                max_value=min(60, max_enc),
                value=min(15, max_enc),
                step=1,
                key="top_dates_slider",
            )

            # ‚úÖ FIXED SETTINGS (tanpa slider enc_window)
            max_pred = int(params.get("max_prediction_length", 1) or 1)
            buffer = 60  # aman untuk rolling/lag/filter internal
            required_n = min(len(df_model), max_enc + max_pred + buffer)

            with st.spinner("Menghitung temporal importance (attention)‚Ä¶"):
                try:
                    df_interp = df_model.tail(required_n).copy()
                    df_interp = align_df_to_dataset_params(df_interp, params)
                    temp = get_temporal_importance_table_interpret_output(model, params, df_interp)
                except Exception as e:
                    st.error("Gagal menghitung temporal importance.")
                    st.exception(e)
                    temp = None

            if temp is None or temp.empty:
                st.warning("Temporal importance tidak tersedia (data window tidak cukup / filter dataset menghapus semua entry).")
            else:
                st.caption(f"Model max_encoder_length: **{max_enc}** | Ditampilkan: **{len(temp)}** tanggal terakhir")

                # Chart
                st.subheader("Attention over Time (Encoder Window)")
                if _HAS_PLOTLY:
                    figt = go.Figure()
                    figt.add_trace(go.Scatter(
                        x=temp["Date"],
                        y=temp["Temporal Importance (%)"],
                        mode="lines",
                        name="Temporal Importance (%)"
                    ))
                    figt.update_layout(
                        height=360,
                        margin=dict(l=10, r=10, t=30, b=10),
                        xaxis_title="Date (history)",
                        yaxis_title="Importance (%)",
                        template="plotly_dark",
                    )
                    st.plotly_chart(figt, use_container_width=True, key=f"att_chart_{len(temp)}_{top_dates}")
                else:
                    st.line_chart(temp.set_index("Date")["Temporal Importance (%)"], height=360)

                # Top crucial dates (ini yang dipengaruhi slider)
                st.subheader("Top Crucial Dates")
                top_tbl = temp.sort_values("Temporal Importance", ascending=False).head(top_dates).copy()
                top_tbl["Date"] = pd.to_datetime(top_tbl["Date"]).dt.date
                top_tbl = top_tbl[["Date", "Temporal Importance (%)"]].reset_index(drop=True)
                top_tbl.index = top_tbl.index + 1
                top_tbl.index.name = "No"
                st.dataframe(top_tbl, use_container_width=True)

            
fig.update_layout(template="plotly_dark")

# ---------- Raw data ----------
with tab_data:
    st.subheader("Latest rows (model dataset)")
    st.dataframe(df.tail(60), use_container_width=True, hide_index=True)

